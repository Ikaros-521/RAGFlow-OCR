#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶å‘OCRæ€§èƒ½åˆ†æå™¨
åˆ†æå¹¶å‘æµ‹è¯•ç»“æœï¼Œç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šå’Œå¯¹æ¯”å›¾è¡¨
æ”¯æŒæ˜¾å­˜ä½¿ç”¨è¶‹åŠ¿åˆ†æå’Œæ€§èƒ½æå‡è®¡ç®—
"""

import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime
import sys

# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç 
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)


class ConcurrentPerformanceAnalyzer:
    """å¹¶å‘æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.results_data = []
        self.plt_style = 'seaborn-v0_8'
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        try:
            plt.style.use(self.plt_style)
        except:
            plt.style.use('default')
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    
    def load_results(self, json_file):
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    self.results_data = data
                else:
                    self.results_data = [data]
            print(f"âœ… å·²åŠ è½½ {len(self.results_data)} ä¸ªæµ‹è¯•ç»“æœ")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def format_memory(self, bytes_value):
        """æ ¼å¼åŒ–å†…å­˜å¤§å°"""
        if bytes_value == 0:
            return "0 B"
        
        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        size = float(bytes_value)
        
        while size >= 1024 and unit_index < len(units) - 1:
            size /= 1024
            unit_index += 1
            
        return f"{size:.2f} {units[unit_index]}"
    
    def analyze_performance(self):
        """åˆ†ææ€§èƒ½æ•°æ®"""
        if not self.results_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return None
        
        analysis = {
            'summary': self._get_summary_stats(),
            'comparison': self._compare_modes(),
            'scalability': self._analyze_scalability(),
            'gpu_usage': self._analyze_gpu_usage(),
            'throughput_analysis': self._analyze_throughput()
        }
        
        return analysis
    
    def _get_summary_stats(self):
        """è·å–æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        summary = {
            'total_tests': len(self.results_data),
            'modes_tested': list(set(r.get('mode', 'unknown') for r in self.results_data)),
            'total_images_processed': sum(r.get('total_images', 0) for r in self.results_data),
            'total_successful': sum(r.get('successful_count', 0) for r in self.results_data),
            'total_failed': sum(r.get('failed_count', 0) for r in self.results_data),
            'avg_throughput': np.mean([r.get('throughput', 0) for r in self.results_data]),
            'max_throughput': max(r.get('throughput', 0) for r in self.results_data),
            'avg_success_rate': np.mean([r.get('successful_count', 0)/max(r.get('total_images', 1), 1) for r in self.results_data])
        }
        return summary
    
    def _compare_modes(self):
        """å¯¹æ¯”ä¸åŒå¹¶å‘æ¨¡å¼"""
        mode_stats = {}
        
        for result in self.results_data:
            mode = result.get('mode', 'unknown')
            workers = result.get('max_workers', 1)
            
            key = f"{mode}_{workers}"
            
            if key not in mode_stats:
                mode_stats[key] = {
                    'mode': mode,
                    'workers': workers,
                    'throughputs': [],
                    'processing_times': [],
                    'success_rates': [],
                    'gpu_efficiencies': [],
                    'total_times': []
                }
            
            mode_stats[key]['throughputs'].append(result.get('throughput', 0))
            mode_stats[key]['processing_times'].append(result.get('avg_processing_time', 0))
            mode_stats[key]['success_rates'].append(result.get('successful_count', 0) / max(result.get('total_images', 1), 1))
            mode_stats[key]['total_times'].append(result.get('total_time', 0))
            
            # GPUæ•ˆç‡åˆ†æ
            if gpu_info := result.get('gpu_info', {}).get('end'):
                mode_stats[key]['gpu_efficiencies'].append(gpu_info.get('utilization', 0))
        
        # è®¡ç®—å¹³å‡å€¼
        comparison = {}
        for key, stats in mode_stats.items():
            comparison[key] = {
                'mode': stats['mode'],
                'workers': stats['workers'],
                'avg_throughput': np.mean(stats['throughputs']),
                'std_throughput': np.std(stats['throughputs']),
                'avg_processing_time': np.mean(stats['processing_times']),
                'avg_success_rate': np.mean(stats['success_rates']),
                'avg_total_time': np.mean(stats['total_times']),
                'avg_gpu_efficiency': np.mean(stats['gpu_efficiencies']) if stats['gpu_efficiencies'] else 0
            }
        
        return comparison
    
    def _analyze_scalability(self):
        """åˆ†æå¯æ‰©å±•æ€§"""
        scalability_data = {}
        
        for result in self.results_data:
            mode = result.get('mode', 'unknown')
            workers = result.get('max_workers', 1)
            
            if mode not in scalability_data:
                scalability_data[mode] = []
            
            scalability_data[mode].append({
                'workers': workers,
                'throughput': result.get('throughput', 0),
                'total_time': result.get('total_time', 0),
                'efficiency': result.get('concurrent_efficiency', 0)
            })
        
        # æŒ‰å·¥ä½œçº¿ç¨‹æ•°æ’åº
        for mode in scalability_data:
            scalability_data[mode].sort(key=lambda x: x['workers'])
        
        return scalability_data
    
    def _analyze_gpu_usage(self):
        """åˆ†æGPUä½¿ç”¨æƒ…å†µ"""
        gpu_data = []
        
        for result in self.results_data:
            gpu_info = result.get('gpu_info', {})
            if not gpu_info:
                continue
            
            start_mem = gpu_info.get('start', {})
            end_mem = gpu_info.get('end', {})
            peak_mem = gpu_info.get('peak_memory', 0)
            
            gpu_data.append({
                'mode': result.get('mode', 'unknown'),
                'workers': result.get('max_workers', 1),
                'start_allocated': start_mem.get('allocated', 0) if start_mem else 0,
                'end_allocated': end_mem.get('allocated', 0) if end_mem else 0,
                'peak_allocated': peak_mem,
                'memory_increase': gpu_info.get('total_memory_increase', 0),
                'total_images': result.get('total_images', 0),
                'utilization': end_mem.get('utilization', 0) if end_mem else 0
            })
        
        return gpu_data
    
    def _analyze_throughput(self):
        """åˆ†æååé‡"""
        throughput_data = []
        
        for result in self.results_data:
            throughput_data.append({
                'mode': result.get('mode', 'unknown'),
                'workers': result.get('max_workers', 1),
                'throughput': result.get('throughput', 0),
                'text_throughput': result.get('text_throughput', 0),
                'success_rate': result.get('successful_count', 0) / max(result.get('total_images', 1), 1),
                'total_images': result.get('total_images', 0)
            })
        
        return throughput_data
    
    def create_performance_report(self, output_dir="benchmark_reports"):
        """åˆ›å»ºæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if not self.results_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(exist_ok=True)
        
        print(f"ğŸ“Š ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Šåˆ°: {output_dir}")
        
        # åˆ†ææ•°æ®
        analysis = self.analyze_performance()
        
        # ç”Ÿæˆå„ç§å›¾è¡¨
        self._create_summary_plots(analysis, output_dir)
        self._create_comparison_plots(analysis, output_dir)
        self._create_scalability_plots(analysis, output_dir)
        self._create_gpu_usage_plots(analysis, output_dir)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._create_text_report(analysis, output_dir)
        
        print(f"âœ… æ€§èƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    
    def _create_summary_plots(self, analysis, output_dir):
        """åˆ›å»ºæ€»ç»“å›¾è¡¨"""
        # æ€»ä½“æ€§èƒ½å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('OCRå¹¶å‘æ€§èƒ½æ€»ä½“åˆ†æ', fontsize=16, fontweight='bold')
        
        # ååé‡å¯¹æ¯”
        comparison = analysis['comparison']
        modes = [f"{data['mode']}_{data['workers']}w" for data in comparison.values()]
        throughputs = [data['avg_throughput'] for data in comparison.values()]
        
        axes[0, 0].bar(modes, throughputs, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('å¹³å‡ååé‡å¯¹æ¯”')
        axes[0, 0].set_ylabel('å›¾ç‰‡å¤„ç†/ç§’')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # æˆåŠŸç‡å¯¹æ¯”
        success_rates = [data['avg_success_rate'] * 100 for data in comparison.values()]
        axes[0, 1].bar(modes, success_rates, color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('æˆåŠŸç‡å¯¹æ¯”')
        axes[0, 1].set_ylabel('æˆåŠŸç‡ (%)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # GPUæ•ˆç‡å¯¹æ¯”
        gpu_efficiency = [data['avg_gpu_efficiency'] * 100 for data in comparison.values()]
        axes[1, 0].bar(modes, gpu_efficiency, color='orange', alpha=0.7)
        axes[1, 0].set_title('GPUåˆ©ç”¨ç‡å¯¹æ¯”')
        axes[1, 0].set_ylabel('åˆ©ç”¨ç‡ (%)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # å¤„ç†æ—¶é—´å¯¹æ¯”
        processing_times = [data['avg_processing_time'] for data in comparison.values()]
        axes[1, 1].bar(modes, processing_times, color='lightcoral', alpha=0.7)
        axes[1, 1].set_title('å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”')
        axes[1, 1].set_ylabel('æ—¶é—´ (ç§’)')
        axes[1, 1].tick_params(axis='xes', rotation=45)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/summary_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_comparison_plots(self, analysis, output_dir):
        """åˆ›å»ºå¯¹æ¯”å›¾è¡¨"""
        comparison = analysis['comparison']
        
        # æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        metrics = ['avg_throughput', 'avg_success_rate', 'avg_gpu_efficiency']
        metric_labels = ['ååé‡', 'æˆåŠŸç‡', 'GPUæ•ˆç‡']
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆåœ†åœˆ
        
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        
        for i, (key, data) in enumerate(comparison.items()):
            values = []
            for metric in metrics:
                if metric == 'avg_throughput':
                    # å½’ä¸€åŒ–ååé‡ (å‡è®¾æœ€å¤§å€¼ä¸º100)
                    values.append(min(data[metric] / 100, 1))
                elif metric == 'avg_success_rate':
                    values.append(data[metric])
                elif metric == 'avg_gpu_efficiency':
                    values.append(data[metric])
            
            values += values[:1]  # é—­åˆåœ†åœˆ
            ax.plot(angles, values, 'o-', linewidth=2, label=key, color=colors[i % len(colors)])
            ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metric_labels)
        ax.set_ylim(0, 1)
        ax.set_title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾å¯¹æ¯”')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/radar_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_scalability_plots(self, analysis, output_dir):
        """åˆ›å»ºå¯æ‰©å±•æ€§å›¾è¡¨"""
        scalability = analysis['scalability']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('å¯æ‰©å±•æ€§åˆ†æ', fontsize=16, fontweight='bold')
        
        # æŒ‰æ¨¡å¼åˆ†ç»„ç»˜åˆ¶
        for mode, modes in scalability.items():
            workers = [d['workers'] for d in modes]
            throughputs = [d['throughput'] for d in modes]
            times = [d['total_time'] for d in modes]
            efficiencies = [d['efficiency'] for d in modes]
            
            # ååé‡æ‰©å±•
            axes[0, 0].plot(workers, throughputs, 'o-', label=f'{mode}', linewidth=2)
            
            # æ€»è€—æ—¶
            axes[0, 1].plot(workers, times, 's-', label=f'{mode}', linewidth=2)
            
            # æ•ˆç‡
            axes[1, 0].plot(workers, efficiencies, '^-', label=f'{mode}', linewidth=2)
            
            # æ‰©å±•å€æ•° (ç›¸å¯¹äºå•çº¿ç¨‹)
            if len(throughputs) > 0:
                speedup = [t/throughputs[0] if throughputs[0] > 0 else 0 for t in throughputs]
                axes[1, 1].plot(workers, speedup, 'd-', label=f'{mode}', linewidth=2)
        
        axes[0, 0].set_title('ååé‡éšçº¿ç¨‹æ•°å˜åŒ–')
        axes[0, 0].set_xlabel('å·¥ä½œçº¿ç¨‹æ•°')
        axes[0, 0].set_ylabel('ååé‡ (å›¾ç‰‡/ç§’)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].set_title('æ€»è€—æ—¶éšçº¿ç¨‹æ•°å˜åŒ–')
        axes[0, 1].set_xlabel('å·¥ä½œçº¿ç¨‹æ•°')
        axes[0, 1].set_ylabel('æ€»è€—æ—¶ (ç§’)')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        axes[1, 0].set_title('å¹¶å‘æ•ˆç‡éšçº¿ç¨‹æ•°å˜åŒ–')
        axes[1, 0].set_xlabel('å·¥ä½œçº¿ç¨‹æ•°')
        axes[1, 0].set_ylabel('æ•ˆç‡')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        axes[1, 1].set_title('åŠ é€Ÿæ¯”éšçº¿ç¨‹æ•°å˜åŒ–')
        axes[1, 1].set_xlabel('å·¥ä½œçº¿ç¨‹æ•°')
        axes[1, 1].set_ylabel('åŠ é€Ÿæ¯”')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/scalability_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_gpu_usage_plots(self, analysis, output_dir):
        """åˆ›å»ºGPUä½¿ç”¨æƒ…å†µå›¾è¡¨"""
        gpu_data = analysis['gpu_usage']
        
        if not gpu_data:
            print("âš ï¸ æ²¡æœ‰GPUä½¿ç”¨æ•°æ®ï¼Œè·³è¿‡GPUå›¾è¡¨ç”Ÿæˆ")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('GPUä½¿ç”¨æƒ…å†µåˆ†æ', fontsize=16, fontweight='bold')
        
        # å‡†å¤‡æ•°æ®
        df = pd.DataFrame(gpu_data)
        
        # GPUå†…å­˜ä½¿ç”¨å¯¹æ¯”
        mode_workers = [f"{row['mode']}_{row['workers']}" for _, row in df.iterrows()]
        
        axes[0, 0].bar(mode_workers, df['peak_allocated'], color='lightblue', alpha=0.7)
        axes[0, 0].set_title('å³°å€¼GPUå†…å­˜ä½¿ç”¨')
        axes[0, 0].set_ylabel('å†…å­˜ (å­—èŠ‚)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # GPUå†…å­˜å¢é•¿
        axes[0, 1].bar(mode_workers, df['memory_increase'], color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('GPUå†…å­˜å¢é•¿')
        axes[0, 1].set_ylabel('å†…å­˜å¢é•¿ (å­—èŠ‚)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # GPUåˆ©ç”¨ç‡
        axes[1, 0].bar(mode_workers, df['utilization'] * 100, color='orange', alpha=0.7)
        axes[1, 0].set_title('GPUåˆ©ç”¨ç‡')
        axes[1, 0].set_ylabel('åˆ©ç”¨ç‡ (%)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # æ¯å¼ å›¾ç‰‡çš„å¹³å‡GPUå†…å­˜ä½¿ç”¨
        avg_mem_per_image = df['peak_allocated'] / df['total_images']
        axes[1, 1].bar(mode_workers, avg_mem_per_image, color='lightcoral', alpha=0.7)
        axes[1, 1].set_title('æ¯å¼ å›¾ç‰‡å¹³å‡GPUå†…å­˜ä½¿ç”¨')
        axes[1, 1].set_ylabel('å†…å­˜/å›¾ç‰‡ (å­—èŠ‚)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/gpu_usage_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # GPUå†…å­˜ä½¿ç”¨è¶‹åŠ¿å›¾
        if len(gpu_data) > 1:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            workers_list = list(set(d['workers'] for d in gpu_data))
            
            for mode in set(d['mode'] for d in gpu_data):
                mode_data = [d for d in gpu_data if d['mode'] == mode]
                mode_data.sort(key=lambda x: x['workers'])
                
                workers = [d['workers'] for d in mode_data]
                peak_mem = [d['peak_allocated'] for d in mode_data]
                
                ax.plot(workers, peak_mem, 'o-', label=f'{mode}', linewidth=2)
            
            ax.set_title('GPUå†…å­˜ä½¿ç”¨éšçº¿ç¨‹æ•°å˜åŒ–')
            ax.set_xlabel('å·¥ä½œçº¿ç¨‹æ•°')
            ax.set_ylabel('å³°å€¼GPUå†…å­˜ä½¿ç”¨ (å­—èŠ‚)')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/gpu_memory_trend.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_text_report(self, analysis, output_dir):
        """åˆ›å»ºæ–‡æœ¬æŠ¥å‘Š"""
        report_file = f"{output_dir}/performance_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# OCRå¹¶å‘æ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ¦‚è¿°
            f.write("## æµ‹è¯•æ¦‚è¿°\n\n")
            summary = analysis['summary']
            f.write(f"- æ€»æµ‹è¯•æ¬¡æ•°: {summary['total_tests']}\n")
            f.write(f"- æµ‹è¯•æ¨¡å¼: {', '.join(summary['modes_tested'])}\n")
            f.write(f"- æ€»å¤„ç†å›¾ç‰‡æ•°: {summary['total_images_processed']}\n")
            f.write(f"- æˆåŠŸç‡: {summary['avg_success_rate']*100:.1f}%\n")
            f.write(f"- å¹³å‡ååé‡: {summary['avg_throughput']:.2f} å›¾ç‰‡/ç§’\n")
            f.write(f"- æœ€å¤§ååé‡: {summary['max_throughput']:.2f} å›¾ç‰‡/ç§’\n\n")
            
            # æ¨¡å¼å¯¹æ¯”
            f.write("## å¹¶å‘æ¨¡å¼å¯¹æ¯”\n\n")
            f.write("| æ¨¡å¼ | çº¿ç¨‹æ•° | å¹³å‡ååé‡ | å¹³å‡æˆåŠŸç‡ | GPUæ•ˆç‡ |\n")
            f.write("|------|--------|------------|------------|----------|\n")
            
            comparison = analysis['comparison']
            for key, data in comparison.items():
                f.write(f"| {data['mode']} | {data['workers']} | "
                       f"{data['avg_throughput']:.2f} | "
                       f"{data['avg_success_rate']*100:.1f}% | "
                       f"{data['avg_gpu_efficiency']*100:.1f}% |\n")
            
            # GPUä½¿ç”¨åˆ†æ
            f.write("\n## GPUä½¿ç”¨æƒ…å†µåˆ†æ\n\n")
            gpu_data = analysis['gpu_usage']
            if gpu_data:
                f.write("| é…ç½® | å¹³å‡GPUåˆ©ç”¨ç‡ | å³°å€¼å†…å­˜ä½¿ç”¨ | å†…å­˜å¢é•¿ |\n")
                f.write("|------|---------------|--------------|----------|\n")
                
                for gpu_info in gpu_data:
                    f.write(f"| {gpu_info['mode']}_{gpu_info['workers']}w | "
                           f"{gpu_info['utilization']*100:.1f}% | "
                           f"{self.format_memory(gpu_info['peak_allocated'])} | "
                           f"{self.format_memory(gpu_info['memory_increase'])} |\n")
            else:
                f.write("æ— GPUä½¿ç”¨æ•°æ®\n")
            
            # å¯æ‰©å±•æ€§åˆ†æ
            f.write("\n## å¯æ‰©å±•æ€§åˆ†æ\n\n")
            scalability = analysis['scalability']
            
            for mode, data in scalability.items():
                f.write(f"### {mode} æ¨¡å¼\n\n")
                f.write("| çº¿ç¨‹æ•° | ååé‡ | åŠ é€Ÿæ¯” | æ•ˆç‡ |\n")
                f.write("|--------|--------|--------|------|\n")
                
                base_throughput = data[0]['throughput'] if data else 1
                
                for d in data:
                    speedup = d['throughput'] / base_throughput if base_throughput > 0 else 1
                    f.write(f"| {d['workers']} | {d['throughput']:.2f} | "
                           f"{speedup:.2f}x | {d['efficiency']:.2f} |\n")
                
                f.write("\n")
            
            # å»ºè®®å’Œç»“è®º
            f.write("## æ€§èƒ½å»ºè®®\n\n")
            
            best_throughput = max(comparison.values(), key=lambda x: x['avg_throughput'])
            best_efficiency = max(comparison.values(), key=lambda x: x['avg_gpu_efficiency'])
            
            f.write(f"### æ¨èé…ç½®\n")
            f.write(f"- **æœ€é«˜ååé‡**: {best_throughput['mode']} æ¨¡å¼ ({best_throughput['workers']} çº¿ç¨‹)\n")
            f.write(f"- **æœ€é«˜GPUæ•ˆç‡**: {best_efficiency['mode']} æ¨¡å¼ ({best_efficiency['workers']} çº¿ç¨‹)\n\n")
            
            f.write("### æ€§èƒ½ä¼˜åŒ–å»ºè®®\n")
            
            # åŸºäºæ•°æ®çš„å»ºè®®
            thread_mode_results = [d for d in comparison.values() if d['mode'] == 'thread']
            process_mode_results = [d for d in comparison.values() if d['mode'] == 'process']
            
            if thread_mode_results and process_mode_results:
                avg_thread_throughput = np.mean([d['avg_throughput'] for d in thread_mode_results])
                avg_process_throughput = np.mean([d['avg_throughput'] for d in process_mode_results])
                
                if avg_thread_throughput > avg_process_throughput:
                    f.write("- æ¨èä½¿ç”¨çº¿ç¨‹å¹¶å‘æ¨¡å¼ï¼Œç›¸æ¯”è¿›ç¨‹æ¨¡å¼æ€§èƒ½æ›´ä½³\n")
                else:
                    f.write("- æ¨èä½¿ç”¨è¿›ç¨‹å¹¶å‘æ¨¡å¼ï¼Œç›¸æ¯”çº¿ç¨‹æ¨¡å¼æ€§èƒ½æ›´ä½³\n")
            
            # GPUä½¿ç”¨å»ºè®®
            if gpu_data:
                avg_gpu_utilization = np.mean([g['utilization'] for g in gpu_data])
                if avg_gpu_utilization < 0.3:
                    f.write("- GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯è€ƒè™‘å¢åŠ å¹¶å‘æ•°é‡æˆ–æ¨¡å‹ä¼˜åŒ–\n")
                elif avg_gpu_utilization > 0.8:
                    f.write("- GPUåˆ©ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®é€‚å½“å‡å°‘å¹¶å‘æ•°é‡é¿å…æ˜¾å­˜æº¢å‡º\n")
                else:
                    f.write("- GPUåˆ©ç”¨ç‡é€‚ä¸­ï¼Œå½“å‰é…ç½®è¾ƒä¸ºåˆç†\n")
        
        print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    parser = argparse.ArgumentParser(description="OCRå¹¶å‘æ€§èƒ½åˆ†æå™¨")
    parser.add_argument('json_file', help='æµ‹è¯•ç»“æœJSONæ–‡ä»¶å')
    parser.add_argument('--output', '-o', default='benchmark_reports', help='è¾“å‡ºæŠ¥å‘Šç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ“Š OCRå¹¶å‘æ€§èƒ½åˆ†æå™¨")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ConcurrentPerformanceAnalyzer()
    
    # åŠ è½½ç»“æœ
    if not analyzer.load_results(args.json_file):
        return
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analyzer.create_performance_report(args.output)
    
    print(f"ğŸ‰ æ€§èƒ½åˆ†æå®Œæˆ! æŸ¥çœ‹ {args.output} ç›®å½•è·å–è¯¦ç»†æŠ¥å‘Š")


if __name__ == "__main__":
    main()
